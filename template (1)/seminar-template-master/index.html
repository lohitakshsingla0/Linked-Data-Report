<!DOCTYPE html>
<html>
<head>
  <title>Seminar Web Engineering in Summer Semester 2022 - Extracting Contentual Knowledge from Linked Data</title>
  <link rel="stylesheet" type="text/css" href="main.css"/>
  <link href='https://fonts.googleapis.com/css?family=Source+Serif+Pro:400,600,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
</head>
<body>
    <header>
        <h2>Seminar Web Engineering in Summer Semester 2022</h2>
		<h1>Extracting Contentual Knowledge From Linked Data</h1>
        <h2 class="author">Anne Seitz</h2>
		<h3 class="affiliation">
			Master's Student of Web Engineering<br/>
			Technical University Of Chemnitz<br/>
			Chemnitz, Germany
		</h3>
    </header>
    <section>
    	<h2>1. Introduction</h2>
        <p> 
    		The steady development drift of Linked Data (LD) is broadening the potential expansion of datasets for improving the search engine available on the web, where, the accessibility of information in Semantic Web groups such as RDF and OWL has also expanded. By the by, the information that is right now accessible constitutes hfair a division of existing data that can be uncovered and conveyed as RDF and OWL. Current investigation and built-up Web look firms like Google and Powerset are advantages of utilizing unequivocal semantics and LD to refine look results.
    	</p>
    	<p>
    		As the Net of Information, imagined by Tim Berners Lee1, gains energy, the request to extricate information and to “triplify” information is consistently expanding, particularly within the ranges of commerce, science, and government. This “triplication” handle, be that as it may, is due to the heterogeneity of information and information models challenging. Even though apparatuses exist to back the era of RDF from bequest sources, a few deterrents stay for computerized approaches. Such obstacles and cost factors include in particular: identification of private and public data, proper reuse of existing vocabularies, missing schema descriptions, and URI generation. 
    	</p>
    	<p>
    		The best source of LD is Wikipedia, which has it own DBpedia where all the linked data keywords are stored in a structured way which is accessible via various tools and methods. The is no manual extraction of data from normal text to RDF format, hence we will discuss the defination of knowledge extraction first and understand the concept. Further, we how to extract Linke data frm structured sources and the terms used in it.
    	</p>
	</section>
    <section>
    	<h2> 2. Knowledge Extraction</h2>
    	<h3> 2.1. Defination</h3>
    		
    	<p>
            Knowledge extraction is the creation of information from organized (social databases, XML) and unstructured (content, archives, pictures) sources. The coming about information has to be in a machine-readable and machine-interpretable arrange
            and must speak to information in a way that encourages inferencing. In spite of the fact that it is deliberately comparable to data extraction (NLP) and ETL (information distribution center), the most measure is that the extraction result goes past the creation of organized data or the change into a social pattern. It requires either the reuse of existing formal information (reusing identifiers or ontologies) or the era of a pattern based on the source information.
            <a href="#r3">[3]</a>.
    	</p>
    	<p>
            In show disdain toward of the reality that the term data extraction is broadly utilized inside the composing, no studies exist that have succeeded in making a framework to cover unstructured as well as organized sources or allow a clear definition of the basic “triplification“ get ready and the required prerequisites. Other frameworks and overviews as well as individual approaches especially require the taking after points such as 
            clear boundaries to existing examine districts such as Information Extraction (Content Mining), Extract-Transform-Load (Data Stockroom) and Cosmology Learning have not in any case been built up. Such boundaries are characterized by expressing unmistakable criteria conjointly by demonstrating the meaning of information w.r.t. the foremost data appear of our concern, RDF and OWL (see another thing). 
            The next point will be The thought that ”knowledge“ is removed has not been well-defined. In spite of the fact that, RDF and OWL can serve as data representation formalisms, the simple utilization of RDF/OWL as a organize can not satisfactorily characterize the thought of ”knowledge“. The foremost questions are: “What is the result of a triplification process?” “Structured data or talked to knowledge?”, “When does organized data gotten to be knowledge?”.
            And then in show disdain toward of the reality that, the extend of extraction of RDF from social databases has been been examined broadly, approaches were scarcely comparable to extraction procedures utilized on other sources, in this way maintaining a strategic distance from theory.
            In the last most individual approaches were driven by uncommonly specific utilize cases that came with a specific data source and required the alter into RDF and, hence, are lost a more common see on the issue. The foremost questions were: “What are the properties of such a transformation?” and “How do they differentiate from past efforts?” 
            <a href="#r7">[7]</a>.
    	</p>
    	<p>
            For the RDF and the OWL information demonstrate, able to distinguish two distinctive criteria for distinguishing Information Extraction forms: Reusing identifiers. (1) generated RDF properties are universally special and have a well-defined meaning. If de-facto standard vocabularies, for case FOAF3 , are reused, at that point the extracted RDF can be unambiguously related to other information within the Web of Information. (2) owl:sameAs or owl:equivalentClass connecting of extricated substances to existing entities within the Web of Information.
            <a href="#r8">[8]</a>.
    	</p>
        <p>
            Construction era. Construction (or Philosophy) era from bequest sources is a weaker shape of information extraction when compared to reusing identifiers. In spite of the fact that pecking orders [4] or OWL/DL maxims of shifting expressivity are generated (see [3] for era of subclass relations from databases and [13] for generating expressive maxims from content) no disambiguation is made by connecting the newly created classes to existing ontologies. The connection of the created scientific classification of terms remains vague and requires strategies from the field of cosmology matching to ended up portion of the worldwide arrange of information.
            <a href="#r8">[8]</a>. 
        </p>	
        <figure>
        <pre>
function createAndSendDocument() {
  // Create a new Google Doc named 'Hello, world!'
  var doc = DocumentApp.create('Hello, world!');

  // Access the body of the document, then add a paragraph.
  doc.getBody().appendParagraph('This document was created ');

  // Get the URL of the document.
  var url = doc.getUrl();

  // Get the email address of the active user - that's you.
  var email = Session.getActiveUser().getEmail();

  // Get the name of the document to use as an email subject line.
  var subject = doc.getName();

  // Append a new string to the "url" variable to use as an email.
  var body = 'Link to your doc: ' + url;

  // Send yourself an email with a link to the document.
  GmailApp.sendEmail(email, subject, body);
}
        </pre>
        <figcaption>
            <strong>Listing 1</strong>: Beispiel Google Script: Spreadsheet erstellen und Link per Mail senden <a href="#r7">[7]</a>
        
        </figcaption>
        </figure>
    </section>
    <section>
    	<h3>2.2. Approach</h3>
    	<p>
            Connected Information regularly combine sorts and properties characterized either in in-house ontologies, or in broad existing ones e.g. FOAF4 , DC5 or GeoNames6 . It may happen, in spite of the fact that, that in-house ontologies have not been formalized, or are not disclosed. Indeed when the ontologies are accessible, they don't tell which relevant part of them is really utilized in a dataset, and what joins are drawn (through the information) between substances over different ontologies.
            Knowing these points of interest about a dataset could be a pre-condition for assessing its adequateness for being reused in a certain setting, for assessing its substance, for coordination it with other (possibly bequest) information; in other words, for utilizing it. We hypothesize that employing KPs for dissecting and conceivably writing LD addresses this problem. In this paper, we center on questioning a dataset when its lexicon is previously unknown, by continuing as takes after as first 
            we characterize a strategy and an metaphysics for analyzing a dataset and producing a union of it i.e. a measured reflection named dataset information design, that highlights how a dataset information is organized, and what its core information components (e.g. central sorts and related KPs) are; and then 
            we appear how this common strategy and cosmology can be misused for recognizing vital KPs extricated from a dataset for creating prototypical queries, through which we are ready to recover a dataset center information.
    	</p>
    	<p>
            A information design (KP) for a sort in an RDF chart incorporates: (i) the properties by which occurrences of this sort relate to other people; (ii) the sorts of such people for each property. A KP is an invariance across observed information or objects that permits a formal or cognitive translation [4]. A KP implants the key relations that portray a significant piece of information in a certain space of intrigued, comparative to etymological outlines and cognitive schemata.
    	</p>
    	<p>
            A path is an requested type-property arrangement that can be navigated in an RDF chart. Note the utilize of sorts in lieu of their occasions, which instep denote multiple events of the same way. The length of a way is the number of properties included (conceivably indeed with redundancies).
            <a href="#r1">[1]</a>.
    	</p>
    	<p>
            Our approach employments a procedure pointed at displaying, reviewing, and summarizing Connected Information sets, in this manner drawing what we call their information architecture, which depends on the ideas of ways and KPs characterized over. The application of this approach is outlined in Figure 1, and can be synthesized as takes after:
            <p>
                1. Accumulate property utilization insights for a chosen dataset and store them as a box of the knowledge design ontology;
                2. Inquiry the dataset for extricating ways. Store all ways with length up to 4, with their utilization insights, within the information design dataset ;
                3. Distinguish central sorts and central properties based on their frequencies in a key position in ways, i.e. betweenness, and number of instantiations;
                4. Extricate developing KPs based on the dataset’s central sorts and properties;
                5. Select clustering factors among central properties, i.e. those properties involving the same position in a set of ways, and build path clusters;
            </p>
            The following area outlines how the components of a dataset knowledge architecture are built for performing the steps of our strategy.
         <a href="#r4">[4]</a>.
    	</p>
    </section>

    <section>
    	<h2>3. Methods and Datasets</h2>
    	<h3>3.1. The Knowledge Architecture</h3>
    	<p>
            A dataset knowledge design is an ontology that communicates a dataset vocabulary in a secluded way. Its components are chosen based on measures that demonstrate their significance in capturing the center information in a dataset. In other words, it is an deliberation over an RDF chart, which offers a modular view as restricted to the normal “class-property” see given by vocabularies and ontologies, since it is open to questions that are skeptic to particular sorts and properties utilized within the dataset.
            <a href="#r2">[2]</a>.
    	</p>
    	<p>
            To populate it, we review a dataset for (i) the types and properties it employments, (ii) its ordinary ways i.e. type-property sequences, and (iii) quantitative insights approximately their utilization. The information architecture schema is accessible online8 . This formalism permits us to experimentally dissect a dataset engineering through SPARQL inquiries.
    	</p>
    	<p>
            Figure 2 portrays the most substances characterized by the dataset knowledge architecture philosophy. With the assistance of this ontology, we point at determining, in a bottom-up way, the philosophy really utilized for speaking to the data in a LD dataset and create extra useful knowledge around a dataset e.g., its central types. We distinguish (i) the properties used in a dataset triples, and demonstrate them through the class Property; (ii) the types (classes or literals) of the subject and question assets of such triples, and model them through the lesson Sort; and (iii) the typical ways that connect triples in a dataset, and demonstrate them through the class.
            <a href="#r11">[11]</a>.
    	</p>
    	<p>
            A Path is an requested set {T1, p1, ..., pl , Tl+1}, where Ti is a Type, pi is a property, and l is the path length. Each requested subset {Ti , pi , Ti+1} of a Path of length l is called PathElement, and is related with its position i = 1, ..l, within the path.
            <a href="#r2">[2]</a>.
    	</p>
    	<p>
            We at that point characterize four properties portraying PathElement: hasProperty, hasPosition, hasPathElementObjectType and hasPathElementSubjectType. Each Way is associated to an occurrence of PathOccurrencesInDataset, which demonstrates the observed number of events of that way in a Dataset.

            We too characterize the concepts CentralType and CentralProperty, which distinguish the substances capturing most of the information in a dataset. The sort KnowledgePattern is utilized for putting away the rising information designs.
            <a href="#r6">[6]</a>.
    	</p>
	</section>
    <section>
    	<h3>3.2 The Knowledge Dimentions</h3>
        <p>
            The measures that we relate to the knowledge architecture of a dataset to experimentally examine and translate them to back our conclusive articulations. Measures from #Triples to #PathOcc hold for a dataset as an entirety, whereas the others are related to a type or property and can be computed either for one dataset or over numerous datasets. Most of the measures have been computed by combining SPARQL questions and software scripting.
            <a href="#r5">[5]</a>.
    	</p>
    	<p>
            The measures for recognizing the central types and properties of a dataset are additionally showed up. Type betweenness and Property betweenness are rearrangements of centrality measures utilized in graph theory. In show disdain toward of the reality that we do not appear the knowledge design of a dataset as a chart, its structure approximates it through the thought of coordinated ways and based on the exploratory perception that all ways longer than 3 are composed of the watched shorter ways.
        </p>
    	<p>
            Ready to at that point compute betweenness of types by tallying the interest of types as subjects in ways of length 2 at position 2, and betweenness of properties by checking the participation of properties in ways of length 3 in position 2.
            The combination of betweenness values and number of occurrences indicates the esteem of centrality of a type or property for a dataset. Central types and properties are able to capture most of the knowledge communicated in a dataset.
    	</p>
        <p>
            There are many measures to extract knowledge Dimentions such as Triples which indicates number of triples that constitute a dataset and it is computed by summing up the all PropertyUsageInDataset in triples.
            The other measure is Props which indicates number of used properties in a dataset and it is calculated by counting all PropertyUsageInDataset.
            The next is Types which shows number of used types and it is computed by Cardinality of the union set of types related to PathElement with either hasPathElementSubjectType or hasPathElementObjectType. 
            Then we have Paths which indicates the number of observed paths of length 2 to 4 and this is computed by Count paths of length n for n = 2...4.
            In the list next we have PathOcc which indicates observed occurrences of paths up to length 4 and it is calculated as sum of the values of hasNumberOfOccurrences of paths of length n, n = 2...4.
            The next measure we have is Property usage in path which shows Sparseness indicator of a dataset knowledge architecture and it is computed by dividing the number of properties that participates in paths of length n by the total number of used properties, with n = 2...4.
            Then the other one is Type betweenness which indicates The capability of a type to catch meaningful knowledge and it is calculated by counting the number of paths of length 2 in which a type participate in at position 1 with subject role.
            Similar to this, the next we have Property betweenness, it shows The capability of a type to catch meaningful knowledge. It is computed by counting the number of paths of length 3 in which a property participates at position 2.
            And the last one we have Triples for property which indicates Number of triples instantiating a property and computed by getting the  value of hasNumberOfTriples for PropertyUsageInDataset about a property.
            So these are the Measures of dataset measurements and characteristics in terms of number of triples, number of sorts and properties utilized, number of ways of length 2 to 4, number of events of ways of length 2 to 4, and property utilization in ways of length 2 to 4.
        </p>
	</section>
    <section>
    	<h2>4. Demo ownCloud</h2>
    	<h3>4.1. Das Set-Up</h3>
    	<p>
        Die häufigste Empfehlung und einfachste Lösung für Privatpersonen ist die Nutzung eines Raspberry Pis auf dem Raspbian installiert ist <a href="#r12">[12]</a>. Bevor
        die eigentliche ownCloud installiert werden kann, bedarf es noch einiger Konfigurationen, wie zum Beispiel die Installation eines Apache Servers, PHP und
        SQL Datenbank <a href="#r9">[9]</a>. Für das Erreichen der Cloud innerhalb des Heimnetzwerks reicht dies schon, wenn man allerdings auch von außerhalb an die Cloud gelangen
        möchte, empfiehlt sich die Registrierung bei einem Dynamic DNS Dienst und die Installation dessen Clients zum regelmäßigen aktualisieren der IP Adresse.
    </p>
    <p>
        Anschließend kann die neuste Version von ownCloud heruntergeladen und installiert werden. Während des Installationsprozesses hat man die Möglichkeit
        entweder die IP des Raspberrys oder eine registrierte URL einzugeben, über welche die ownCloud dann erreichbar ist. Dazu sollten dann auch die Ports 80 und
        443 im Router zur Weiterleitung freigegeben sein<a href="#r9">[9]</a>.
    </p>
    <p>
        Der nächste Schritt ist bezüglich Sicherheit. Um eine sichere Verbindung über HTTPS zu haben, kann ein selbstsigniertes Zertifikat erzeugt werden. Zwar
        erscheint dann beim Aufruf der Seite die Warnung, dass es sich um keine vertrauenswürdige Verbindung handelt, jedoch ist der Grund dafür ja die selbst
        ausgestellte Signatur. Durch Hinzufügen einer Ausnahme für dieses Zertifikat und abspeichern im Browser ist dennoch eine gewisse Kontrolle über die
        Verbindung gewährt. Sollte in Zukunft beim Aufrufen der Seite diese Warnung wieder erscheinen, obwohl das eigene Zertifikat gespeichert wurde, weiß man
        beispielsweise, dass jemand Fremdes an dem Zertifikat Änderungen vorgenommen hat <a href="#r5">[5]</a>.
    </p>
    <p>
        Als letztes muss nun nur noch der Dynamic DNS Update Client installiert und konfiguriert werden. Nach einem Neustart des Servers und einloggen auf der
        eigenen ownCLoud über das Webinterface ist diese bereit zum Nutzen. Es können weitere Apps eingebunden, eine Synchronisation mit Adressbuch oder Kalender
        eingerichtet und das Aussehen angepasst werden <a href="#r10">[10]</a>.
    </p>
    </section>
    <section>
        <h3>4.2. Funktionsbeispiele</h3>

    <p>
        Ein sehr positiver Aspekt ist, dass man die eigene Cloud auch anderen zugänglich machen kann. Der Admin kann Benutzer anlegen und diese Gruppen sowie
        Rechte zuweisen (siehe Abb.1). Diese können ihre eigenen Daten hochladen, verwalten, teilen und miteinander interagieren.
    </p>
    	<figure>
          <img src="benutzer.png" alt="Liste aller Benutzer"/>
          <figcaption>
            <strong>Abbildung 1</strong>: Liste der eingetragenen Benutzer <a href="#r13">[13]</a>.
          </figcaption>
        </figure>
    <p>
        Beim Teilen gibt es verschiedene Möglichkeiten dieses zu gestalten. Zum einen können Dateien mit ganzen Gruppen oder einzelnen Personen geteilt werden.
        Des Weiteren lassen sich Optionen wie „kann bearbeiten“, „aktualisieren“, „löschen“ und „teilen“ auswählen. Auch das Setzen eines Ablaufdatums ist möglich,
        sowie eine zusätzliche Mitteilung per Email mit der Information, dass etwas mit einem geteilt wurde. Aber auch das Teilen mit Dritten, die nicht zu den
        eingetragenen Benutzern gehören, ist möglich. Hierfür orientiert sich ownCloud an Dropbox oder auch Google, indem es den Link zum kopieren und weiterleiten gibt
        (siehe Abb 2).
    </p>
    <figure>
          <img src="shareoptions.png" alt="Teilen"/>
          <figcaption>
            <strong>Abbildung 2</strong>: Optionen beim Teilen von Daten <a href="#r13">[13]</a>.
          </figcaption>
        </figure>
    <p>
        Gerade in Hinblick auf das Zusammenarbeiten und Teilen von Daten ist die Übersicht der neusten Aktivitäten sehr nützlich. Hier werden alle Aktivitäten, die
        einen selbst betreffen mit zusätzlichen Details angezeigt (siehe Abb.3).
    </p>
    	<figure>
          <img src="activity_c.png" alt="Aktivitätsübersicht"/>
          <figcaption>
            <strong>Abbildung 3</strong>: Übersicht der Aktivität <a href="#r13">[13]</a>.
          </figcaption>
        </figure>
    </section>
    <section>
        <h2>5. Fazit</h2>

    <p>
        Die Vorteile der Nutzung von Cloud Diensten liegt klar auf der Hand: Die Daten sind unabhängig von Standort und Gerät immer abrufbar. Im Falle eines
        Schadens an der Festplatte oder ähnlichem sind zudem diese Daten nicht verloren. Auch vereinfacht es Projektarbeiten. Doch es gibt immer zwei Seiten
        einer Medaille und so sollte sich jeder bewusst sein, dass wenn man seine Daten, seien es persönliche, geschäftliche, belanglose oder sensible Daten, online hochlädt und 
    	auf fremden Servern speichert, die theoretische Möglichkeit besteht, dass jemand Fremdes sich leichter und unter Umständen unbemerkt Zugriff zu diesen Daten beschaffen kann.
    </p>
    <p>
        Große Anbieter wie Dropbox und Google versprechen viel Leistung, gerade auch in Hinblick auf Sicherheit, aber so zeigte sich besonders im letzten Jahr wie
        viel hinter den Kulissen geschieht, ausgehend von der Politik und den Geheimdiensten, von dem Anwender nichts mitbekommen. Die besten
        Verschlüsselungsalgorithmen bei der Übertragung von Daten oder beim Ablegen der selbigen auf den Servern nützen nichts, wenn die Schlüssel von den Firmen
        selbst verwaltet werden und sie auf Verlangen der Regierung diese herausgeben müssen.
    </p>
    <p>
        Eine Alternative ist sicher die zusätzliche lokale Verschlüsselung, so wie sie Boxcryptor anbietet. Oder auch gänzlich auf das Zwischenspeichern auf
        Servern zu verzichten und Daten direkt zu Synchronisieren, so wie BitTorrent Sync es vorschlägt. Die Idee sich seine eigene Cloud aufzusetzen erscheint im
        ersten Moment auch eine interessante Alternative zu sein. Jedoch kann dieser Enthusiasmus schnell verfliegen, denn die Herausforderung diese so sicher wie
        möglich zu gestalten und den gleichen Service zu bieten wie die Vorbilder, ist doch größer als anfänglich gedacht. Viel Freiheit in Gestaltung und
        Umsetzung bedeutet in diesem Fall auch sehr viel Verantwortung, welche nur mit Grundkenntnissen kaum erfüllbar ist.
    </p>
	</section>
	<section class="references">
	   		<h2>6. Literaturverzeichnis</h2>
    		<p class="reference" id="r1">[1] Barth, Dave (2013, August 13). Google Cloud Storage now provides server-side encryption [Online]. Available: <a href="http://googlecloudplatform.blogspot.de/2013/08/google-cloud-storage-now-provides.html">http://googlecloudplatform.blogspot.de/2013/08/google-cloud-storage-now-provides.html</a> (28.06.2014)</p>
    		<p class="reference" id="r2">[2] BitTorrent Sync [Online]. Available: <a href="http://www.bittorentsync.com">http://www.bittorentsync.com</a> (29.06.2014)</p>
    		<p class="reference" id="r3">[3] Borgmann, Moritz et al. "On the Security of Cloud Storage". Fraunhofer Gesellschaft SIT. Darmstadt. SIT Technical Reports. SIT-TR-2012-001.  March 2012</p>
    		<p class="reference" id="r4">[4] Boxcryptor [Online]. Available: <a href="http://www.boxcrypotr.com">http://www.boxcryptor.com</a> (29.06.2014)</p>
    		<p class="reference" id="r5">[5] Connect.de. So Installieren Sie ownCloud auf einem Raspberry Pi [Online]. Available <a href="http://www.connect.de/ratgeber/so-installieren-sie-owncloud-auf-einem-raspberry-pi-1540246.html">http://www.connect.de/ratgeber/so-installieren-sie-owncloud-auf-einem-raspberry-pi-1540246.html</a> (29.06.2014)</p>
    		<p class="reference" id="r6">[6] Donauer, Jürgen  (2013, December 18). Die Cloud Alternative für Linux: Bittorrent Sync [Online]. Available: <a href="http://www.pcwelt.de/ratgeber/Die_Cloud_Alternative_fuer_Linux__Bittorrent_Sync-Datenaustausch_uebers_Internet-8345539.html">http://www.pcwelt.de/ratgeber/Die_Cloud_Alternative_fuer_Linux__Bittorrent_Sync-Datenaustausch_uebers_Internet-8345539.html</a> (29.06.2014)</p>
    		<p class="reference" id="r7">[7] Dropbox [Online]. Available: <a href="http://www.driobox.com">http://www.dropbox.com</a> (29.06.2014)</p>
    		<p class="reference" id="r8"> [8] Google Drive [Online]. Available: <a href="http://www.drive.google.com">http://drive.google.com</a> (29.06.2014)</p>
    		<p class="reference" id="r9"> [9] H., Muhamed (2014, January). Raspberry Pi - ownCloud installieren Anleitung [Online]. Available: <a href="http://linuxwelt.blogspot.de/2014/01/raspberry-pi-owncloud-installieren.html">http://linuxwelt.blogspot.de/2014/01/raspberry-pi-owncloud-installieren.html</a> (02.07.2014)</p>
    		<p class="reference" id="r10"> [10] ownCloud [Online]. Available: <a href="http://www.owncloud.org">http://www.owncloud.org</a> (01.07.2014)</p> 
    		<p class="reference" id="r11">[11] Simonite, Tom (2014, January 17). Sync Your Files without Trusting the Cloud [Online]. Available <a href="http://www.technologyreview.com/news/522516/sync-your-files-without-trusting-the-cloud/">http://www.technologyreview.com/news/522516/sync-your-files-without-trusting-the-cloud/</a> (29.06.2014)</p>
    		<p class="reference" id="r12">[12] Thoma, Jörg (2014, April 15). OwnCloud - Dropbox Alternative fürs Heimnetzwerk [Online]. Available <a href="http://www.golem.de/news/owncloud-dropbox-alternative-fuers-heimnetzwerk-1404-105843.html">http://www.golem.de/news/owncloud-dropbox-alternative-fuers-heimnetzwerk-1404-105843.html</a> (29.06.2014)</p>
    		<p class="reference" id="r13"> [13] eigene aufgesetzte ownCloud </p> 
	</section>
